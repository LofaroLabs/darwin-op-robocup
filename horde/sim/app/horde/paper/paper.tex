\documentclass{aamas2010_cameraReady}
\usepackage{amsmath}

\pdfpagewidth=8.5truein
\pdfpageheight=11truein

\renewcommand\floatpagefraction{.9}
\renewcommand\topfraction{.9}
\renewcommand\bottomfraction{.9}
\renewcommand\textfraction{.1}   
\setcounter{totalnumber}{50}
\setcounter{topnumber}{50}
\setcounter{bottomnumber}{50}

\newcommand\bump{\vspace{10in}}

\usepackage{url}
\begin{document}

% In the original styles from ACM, you would have needed to
% add meta-info here. This is not necessary for AAMAS 2010 as
% the complete copyright information is generated by the cls-files.

% For appropriate information about authors and title written in the
% copyright information, you must use these commands. Note that the copyright
% box is not required for the initial submission.

\AuthorsForCitationInfo{Anonymous}
%\AuthorsForCitationInfo{Sean Luke, Vittorio Amos Ziparo}


\TitleForCitationInfo{Learn to Behave! Rapid Training of Behavior Automata}

\title{Learn to Behave!\\Rapid Training of Behavior Automata}

\sloppy

%\numberofauthors{2}

%\author{
%\alignauthor
%Sean Luke\\
%       \affaddr{Department of Computer Science}\\
%       \affaddr{George Mason University}\\
%       \affaddr{4400 University Drive MSN 4A5}\\
%       \affaddr{Fairfax, VA 22030 USA}\\
%       \email{sean@cs.gmu.edu}
%% 2nd. author
%\alignauthor
%Vittorio Amos Ziparo\\
%       \affaddr{Dipartimento di Informatica e Sistemistica}\\
%       \affaddr{Universit{\`a} di Roma ``La Sapienza''}\\
%       \affaddr{Via Ariosto 25, I-00185}\\
%       \affaddr{Rome, ITALY}\\
%       \email{ziparo@dis.uniroma1.it}
%}

%%%% IMPORTANT NOTE FROM SEAN
%%%% When de-anonymizing, see the example.tex file which also has something that has to be
%%%% uncommented as well. REMINDER.

\numberofauthors{1}
\author{
\alignauthor
By Anonymous\\
       \affaddr{~}\\
       \affaddr{ ~}\\
       \affaddr{~ }\\
       \affaddr{~ }\\
       \email{ ~}\\
}

\maketitle

\begin{abstract}
Programming robot or virtual agent behaviors can be a challenging task, and makes attractive the prospect of automatically learning the behaviors from the actions of a human demonstrator.  However, learning complex behaviors rapidly from a demonstrator may be difficult if they demand a large number of training samples.  We describe an architecture for rapid learning of recurrent behaviors from demonstration.  The architecture is based on deterministic hierarchical finite-state automata (HFAs) with classification algorithms taking the place of the state transition function.  This architecture allows for task decomposition, statefulness, parameterized features and behaviors, per-behavior feature set customization, and storage of learned behaviors in libraries to be used later on as elements in more complex behaviors.  We describe the system, then illustrate its application in a simple, but nontrivial,  foraging task involving multiple behaviors.

\end{abstract}

% Note that the category section should be completed after reference to the ACM Computing Classification Scheme available at
% http://www.acm.org/about/class/1998/.

\category{I.2.6}{Artificial Intelligence}{Learning}

%A category including the fourth, optional field follows...
%\category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]

%General terms should be selected from the following 16 terms: Algorithms, Management, Measurement, Documentation, Performance, Design, Economics, Reliability, Experimentation, Security, Human Factors, Standardization, Languages, Theory, Legal Aspects, Verification.

\terms{Algorithms, Design, Human Factors}

%Keywords are your own choice of terms you would like the paper to be indexed by.

\keywords{Learning from Demonstration, Hierarchical Finite-state Automata, Agents, Robotics}

\section{Introduction}

Our goal is to enable the rapid, real-time training of complex, stateful agent behaviors.  Agent behavior training has applications in a variety of fields, including 3D animation, game level design, and autonomous robotics.  In these areas, programming custom domain-specific behaviors on-the-fly may not be desirable or possible, and so it is attractive to instead have the agent learn them from a trainer.

One of the challenges facing training, however, is the conflict between the real-time nature of training and the large numbers of samples that may be demanded by a challenging, high-dimensional domain.  It may not be feasible to ask a trainer to perform hundreds of trials to satisfy the needs of a learning algorithm.  Thus, one of our goals is to develop methods to reduce domain complexity, and ideally reduce the number of necessary samples, while not sacrificing the gamut of learnable behaviors.  We do this by taking advantage of domain knowledge in various ways, and thus our method lies somewhere in the middle-ground between explicit programming (that is, specification) and full, unfettered learning.

Our learned agent behaviors take the form of deterministic hierarchical finite-state automata (HFA).  The motivation underlying this choice is twofold. First, HFA are a widely adopted tool for modeling agent and robot behaviors, yet are simple enough to allow the demonstration of our learning approach.  Second, we chose HFAs as they enabled us to do task decomposition.

There are many HFA formulations.  Ours is straightforward: a learned behavior is a standard Moore Machine finite-state automaton, where each state is associated with a certain behavior, and also with a transition function which stipulates, given the current world situation, which state to transition to in the next time step.  There is a start state but no accepting states.  

%A state's behavior can either be a basic behavior hard-coded into the agent itself; or the behavior may itself be another, lower-level finite-state automaton.  In the second case, when in a state \(S\) associated with a lower-level finite-state automaton behavior \(B\), the agent performs the behaviors, and appropriate transitions, of \(B\) until such time that some world condition causes a transition away from \(S\) and to a new higher-level state \(S'\).  At this point \(B\)'s FSA is eliminated and the agent moves on to performing \(S'\).  The HFA may have any number of levels of embedded automata, but recursion is not permitted.

Our approach is to build an HFA iteratively: we allow the user to easily create an HFA based on a current library of behaviors (some of which may themselves be HFAs).  When the HFA is complete, it is added to the library to help build a more complex higher-level HFA.   One can create of course an HFA by coding it by hand: but of interest to us is the ability to {\it learn} the HFA by watching a demonstrator manipulate the agent.  As the agent moves about in the environment, the demonstrator directs it to perform various behaviors (and thus to transition to various new states). Each time the demonstrator requests such a transition, the system records the transition and the current world situation.
% (the feature vector provided the agent).  
At the end of the training period, from these records the system builds, for each state (behavior), a learned transition function indicating under what conditions the agent should transition to new states.  This is essentially a supervised learning task and can employ a variety classification algorithms: at present our learned models take the form of decision trees.

The learning domain for an HFA behavior can obviously be complex and of high dimension, depending on the number of basic behaviors and the dimensionality of the agent's feature vector.  This in turn can require a high number of training sessions to adequately describe the domain.  It is not reasonable to expect a demonstrator to perform that many training sessions, and so it is important to reduce the domain space complexity or training difficulty.  We have done this in three ways:

\begin{itemize}
\item An HFA encourages task decomposition.  Rather than learn one large behavior, the system may be trained on simpler behaviors, which are then composed into a higher-level learned behavior.  This essentially projects the full learning space into multiple lower-dimensional spaces.

\item Feature vector reduction.  Our system allows the user to specify precisely those features he feels are necessary for a given learned HFA, which in turn dramatically reduces the learning space.  Each HFA, including lower-level HFAs, may have its own reduced feature vector.

\item Generalization by parametrization.  All behaviors, including HFAs themselves, may be parameterized with {\it targets}: for example, rather than create a behavior \textsf{go-to-home-base}, we can create a general behavior \textsf{go-to(A)}, and allow for higher-level behaviors to specify the meaning of the target \textsf{A} at a future time.  This can significantly reduce the number of behaviors which must be trained.
\end{itemize}

By employing these complexity-reduction measures, our system ideally enables the rapid construction of complex behaviors, with internal state and a variety of sensor features, in real time entirely by training from demonstration.

The remainder of the paper is laid out as follows.  We begin with a discussion of related work.  We then describe the basic HFA model and our approach to learning the transition functions in the automaton.  We follow this with a training example of a nontrivial foraging behavior, then conclude with a discussion of future directions.

%Our automaton consists of a set \(\mathcal{S} = \{S_1, S_2, ..., S_n\}\) of {\it states}, exactly one of which is active at any time.  Each state corresponds to a {\it behavior} which the agent performs while that state is active.  The HFA also contains a set \(\mathcal{E} = \{E_1, E_2, ..., E_n\}\) of {\it transition edges}, where each edge \(E_i\) is a triple of the form \(\langle S, S', C\rangle\) indicating a current {\it outgoing} state \(S\), a new {\it incoming} state \(S'\), and a {\it condition} \(C\).  Edges control how the HFA transitions from state to state: if \(S\) is presently the active state, and the condition \(C\) occurs in the environment, and \(\langle S, S', C\rangle \in \mathcal{E}\), then the HFA will transition to \(S'\) as its new active state.  We presume that for a given state \(S\), all edges with \(S\) as their outgoing state have disjoint and non-conflicting conditions.  Furthermore we may describe all such edges collectively as a {\it transition function} \(T(S, C)\rightarrow S'\) which maps the current state \(S\) and the current world situation \(C\) into a transition to a new state \(S'\).  One state \(S_0\) is the {\it start state}.

%The finite-state automaton is hierarchical in the following way: the behaviors associated with a given state \(S\) may either be hard-coded {\it basic behaviors} built into the agent itself; or they may themselves be finite-state automata.  When we transition to \(S\), and it is associated with a FSA behavior, we initialize the 


\input{previouswork}
\input{approach}
\input{example}
\input{conclusions}

%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{demonstration}  % sigproc.bib is the name of the Bibliography in this case

%\balancecolumns
\end{document}
