%!TEX root = paper.tex

\section{Conclusion}

In this paper, we have presented an approach for training agent behaviors using a hierarchical deterministic finite state automata model and a classification algorithm, implemented as a variant of the C4.5 algorithm. The main goal of our approach is to enable users to train agents rapidly based on a small number of training examples.  In order to achieve this goal, we trade off learning complexity with training effort, by enabling trainers to decompose the learning task in a hierarchical manner, to learn general parameterized behaviors, and to explicitly select the most appropriate features to use when learning. This in turn reduces the dimensionality of the learning problem. 

We have developed a proof of concept testbed simulator which appears to work well: we can train parameterized, hierarchical behaviors for a variety of tasks in a short period of time.  We are presently deploying the platform to robots in our laboratory.  In the mean time, there are a number of interesting issues that remain to be dealt with.

\paragraph*{Multiple Agents}  Our immediate next goal is to move to training multiple agents.  In the general case, multiagent learning is a much more complex task than single-agent learning, involving game-theoretic issues which may be well outside the scope of the learning facility.  However we believe there are obvious approaches to certain simple multiagent learning scenarios: for example teaching agents to perform actions as {\it homogeneous behavior} groups (perhaps by training an agent with respect to other agents not under his control, but moving them similarly).  Another area of multiple agent training may involve {\it hierarchies of agents,} with certain agents in control of teams of other agents.

\paragraph*{Unlearning} 
There are two major reasons why an agent may make an error.  First, it may have learned poorly due to an insufficient number of examples or unfortunately located examples.  Second, it may have been misled due to {\it bad examples}.  This second situation arises due to errors in the training process, something that's surprisingly easy to do!  When an agent makes a mistake, the user can jump in and correct it immediately, which causes the system to drop back into training mode and add those new examples to the behavior's collection.  However this does {\it not} cause any errant examples to be removed.  Since the agent made an error based not on examples but rather based on the learned function, identifying which examples were improper, and whether to remove them, may prove a challenge.

\paragraph*{Programming versus Training}  We have sought to train agents rather than explicitly code them.  However we also aimed to do so with a minimum of training.  These goals are somewhat in conflict.  To reduce the training necessary, we typically must reduce the problem space complexity and/or dimensionality.  We have so far done so by allowing the user to inject domain knowledge into the problem (via task decomposition, for example, or by explicitly training for certain corner cases).  This is essentially a step towards having the user explicitly declare part of the solution rather than have the learner induce it.  So is this learning or coding?

We think that training of this sort is somewhere in-between: in some sense the learning algorithm is relieving the trainer from having to ``code'' everything himself.  The question worth studying is: how much learning is useful before the number of samples required to learn outweighs the reduced ``coding'' load, so to speak, on the trainer?

\paragraph*{Other Representations}  HFAs cannot straightforwardly do parallelism or planning.  We chose HFAs largely because they were simple enough to make training intuitively feasible.  Now that we've demonstrated this, we wish to examine how to train with other common representations, such as Petri nets or hierarchical task network plans, to demonstrate the generality of the approach.