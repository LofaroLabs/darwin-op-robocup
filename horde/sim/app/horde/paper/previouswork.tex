%!TEX root = paper.tex

\section{Related Work}

Our approach generally fits under the category of {\it learning from demonstration} \cite{DBLP:conf/icml/AtkesonS97}, an overall term for training agent actions by having a human demonstrator perform the action on behalf of the agent.  Because the proper action to perform in a given situation is directly provided to the agent, this is broadly speaking a supervised learning task, though a significant body of research in the topic actually involves reinforcement learning, whereby the demonstrator's actions are converted into a reinforcement signal from which the agent is expected to derive a policy.  The lion's share of learning from demonstration literature comes not from virtual or game agents but from autonomous robotics.  For a large survey of the area, see \cite{argall2009}.

\paragraph*{Learning Plans}
One learning from demonstration area, closely related to our own research, involves the learning of largely directed acyclic graphs of behaviors (essentially plans) from sequences of actions \cite{DBLP:conf/atal/AngrosJRS02,DBLP:journals/tsmc/NicolescuM01,DBLP:conf/atal/NicolescuM03,DBLP:conf/hri/RybskiYSV07}, possibly augmented with sequence iteration \cite{DBLP:conf/iros/VeeraraghavanV08}.  Like our approach, these plans are often parameterizable.

Such plan networks generally have limited or no recurrence: instead they usually tend to be organized as sequences or simultaneous groups of behaviors which activate further behaviors downstream.  This is mostly a feature of the problem being tackled: such plans are largely induced from ordered sequences of actions intended to produce a result.  Since we are training goal-less behaviors rather than plans, our model instead assumes a rich level of recurrence: and for the same reason the specific ordering of actions is less helpful.

\paragraph*{Learning Policies}
Another large body of work in learning from demonstration involves observing a demonstrator perform various actions when in various world situations.  From this the system gleans a set of \(\langle\textit{situation, action}\rangle\) tuples performed and builds a policy function \(\pi(\textit{situation})\rightarrow\textit{action}\) from these tuples.   This can be tackled as a supervised learning task \cite{Bain96aframework,DBLP:journals/ras/BentivegnaAC04,DBLP:conf/hri/CalinonB07,dinerstein07,DBLP:journals/ras/KasperFSP01,DBLP:journals/ras/NakanishiMECSK04}.  However, some literature instead transforms the problem into a reinforcement learning task by providing the learner only with a reinforcement signal based on how closely the learned policy matches the tuples provided by the demonstrator \cite{DBLP:journals/cacm/CoatesAN09,yasutake2}.

Our approach differs from these methods in an important way.  Instead of learning {\it situation\(\rightarrow\)action} rules, our model learns the transition functions of an HFA with predefined internal states, each corresponding to a possible basic behavior.  This enables the demonstrator to differentiate transitions to new behaviors not just based on the current world situation but also the current behavior.  Another, somewhat different use of internal state would be to distinguish between aliased observations of hidden world situations, something which may be accomplished through learning hidden Markov models (for example, \cite{4756000}).


\paragraph*{Hierarchical Models}
The use of hierarchies in robot or agent behaviors is very old indeed, going back as early as Brooks's Subsumption Architecture \cite{brooks}.  Hierarchies are a natural way to achieve layered learning \cite{DBLP:conf/ecml/StoneV00} via task decomposition.  This is a common strategy to simplify the state space: see \cite{4354062} for an example.  While it is possible in these cases to induce the hierarchy itself, usually such methods iteratively compose hierarchies in a bottom-up fashion.

Our HFA model bears some similarity to hierarchical behavior networks such as those for virtual agents \cite{Bindiganavale00dynamicallyaltering} or physical robots \cite{DBLP:conf/atal/NicolescuM02}, in which feed-forward plans are developed, then incorporated as subunits in larger and more complex plans.  In such literature, the actual application of hierarchy to learning from demonstration has been unexpectedly limited.  However, learning from demonstration has been applied more extensively to multi-level reinforcement learning, as in \cite{yasutake1}, albeit with a fixed hierarchy.

\paragraph*{Language Induction}
One cannot mention learning finite state automata without noting that they have a long history in language induction and grammatical inference, with a correspondingly massive literature.  For recent surveys of techniques using automata for grammar induction, see \cite{Parekh00grammarinference, vidal}.  However the goal of this literature is fundamentally different from ours in this paper.  Specifically, in language induction, the learning algorithm is given a set of positive and negative string examples and generates an automaton which induces an underlying language.  Typically these algorithms make no assumptions about the number of states, assume the states are unlabelled, typically assume a small set of transition conditions, and include accepting or rejecting states.  In contrast we are not interested in terminating automata, and seek to induce only the edges among a prespecified set of labeled states, given examples with labelled transitions from state to state.
